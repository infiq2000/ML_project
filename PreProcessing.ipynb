{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trich xuat cac dac trung can su dung va chuyen doi ve dang numpy de xu ly\n",
    "import numpy as np\n",
    "def extract_feature(data, feature_titles):\n",
    "    features = [data[val].values for val in feature_titles]\n",
    "    \n",
    "    return np.stack(features, axis= - 1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape ve 1 cot\n",
    "a = np.random.rand(2,3)\n",
    "b= np.reshape(a, [-1, 1])\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape ve 1 dong\n",
    "a = np.random.rand(2,3)\n",
    "b= np.reshape(a, [1, -1])\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ve phan phoi cua mot bien\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "#Bieu do tan suat\n",
    "data['GrLivArea'].hist(bins=50, ax=ax, density=True, color='red')\n",
    "#Bieu do mat do\n",
    "data['GrLivArea'].plot.density(color='red')\n",
    "#Bieu do mat do\n",
    "X_train['Age_mean'].plot(kind='kde', ax=ax, color='green')\n",
    "#boxplot\n",
    "X_train[[\"GarageYrBlt\", 'GarageYrBlt_median']].boxplot()\n",
    "# gan label cho hinh\n",
    "lines, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(lines, labels, loc='best')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean-median-imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(X_train)\n",
    "imputer.transform(X_test)\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "preprocessing = ColumnTransformer(transformers=[\n",
    "    (\"mean_imputer\",SimpleImputer(strategy='mean'),_list_cac_cot_dung)\n",
    "    (\"median_imputer\",SimpleImputer(strategy=\"median\"), list_cac_cot_dung)\n",
    "],remainder=\"passthrough\")\n",
    "preprocessing.fit(X_train)\n",
    "#phuong thuc\n",
    "preprocessor.transformers\n",
    "\n",
    "preprocessing.named_transformers_['mean_imputer'].statistics_\n",
    "\n",
    "preprocessing.transform(X_train)\n",
    "\n",
    "np.mean(np.isnan(X_train)) # boi vi sklearn chuyen dataframe ve numpy\n",
    "\n",
    "preprocessor.transformers_\n",
    "\n",
    "remainder_cols = [cols_to_use[c] for c in [0, 1, 2, 3, 4, 5]]\n",
    "remainder_cols\n",
    "\n",
    "from sklearn.impute import SimpleImputer, MissingIndicator\n",
    "indicator = MissingIndicator(error_on_new=True, features=\"missing-only\")\n",
    "\n",
    "# we can find the feature names by passing the index to the\n",
    "# list of columns\n",
    "\n",
    "X_train.columns[indicator.features_]\n",
    "\n",
    "tmp = indicator.transform(X_train)\n",
    "\n",
    "indicator_cols = [c+'_NA' for c in X_train.columns[indicator.features_]]\n",
    "\n",
    "# and now we concatenate\n",
    "X_train = pd.concat([\n",
    "    X_train.reset_index(),\n",
    "    pd.DataFrame(tmp, columns = indicator_cols)],\n",
    "    axis=1)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_categorical = [c for c in data.columns if data[c].dtypes=='O']\n",
    "\n",
    "# find numerical variables\n",
    "# those different from object and also excluding the target SalePrice\n",
    "features_numerical = [c for c in data.columns if data[c].dtypes!='O' and c !='SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import Lasso\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer',SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()))\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\",SimpleImputer(strategy='constant',fill_value=\"missing\"))\n",
    "    ('onehot', OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformer = [ \n",
    "    (\"numerical\",numeric_transformer,feature_numerical),\n",
    "    (\"caterorical\",categorical_transformer, features_categorical)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"preprocessor__numerical__imputer__strategy\" : ['mean','median'],\n",
    "    \"preprocessor__categorical__imputer__strategy\":['most_frequent',\"constant\"],\n",
    "    'regressor__alpha':[10,100,200],\n",
    "}\n",
    "clf = Pipeline(steps =[\n",
    "    ('preprocessor',preprocessor),\n",
    "    (\"regressor\",Lasso(max_iter=2000))\n",
    "    ])\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "grid_search = GridSearchCV(clf, param_grid,cv=5,iid=False,n_jobs=-1,scoring='r2')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print((\"best linear regression from grid search: %.3f\"\n",
    "       % grid_search.score(X_train, y_train)))\n",
    "\n",
    "grid_search.cv_results_[\"mean_test_score\"]\n",
    "\n",
    "encoder = OneHotEncoder(categories='auto',\n",
    "                       drop='first', # trả về k-1, sử dụng drop=false để trả về k biến giả\n",
    "                       sparse=False,\n",
    "                       handle_unknown='error') # giúp xử lý nhãn hiếm\n",
    "\n",
    "encoder.fit(X_train.fillna('Missing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(X_train['Neighborhood'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = defaultdict(LabelEncoder)\n",
    "train_transformed = X_train.apply(lambda x: d[x.name].fit_transform(x))\n",
    "\n",
    "# # Using the dictionary to encode future data\n",
    "test_transformed = X_test.apply(lambda x: d[x.name].transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = [\n",
    "    x for x in X_train['Neighborhood'].value_counts().sort_values(ascending=False).head(10).index\n",
    "]\n",
    "\n",
    "for label in top_10:\n",
    "    ## VIẾT CODE Ở ĐÂY:\n",
    "    X_train['Neighborhood' + '_' + label] = np.where(\n",
    "        X_train['Neighborhood'] == label, 1, 0)\n",
    "    \n",
    "    X_test['Neighborhood' + '_' + label] = np.where(X_test[\"Neighborhood\"]==label,1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_top_categories(df, variable, how_many=10):\n",
    "    return [\n",
    "        x for x in df[variable].value_counts().sort_values(\n",
    "            ascending=False).head(how_many).index\n",
    "    ]\n",
    "\n",
    "\n",
    "def one_hot_encode(train, test, variable, top_x_labels):\n",
    "\n",
    "    for label in top_x_labels:\n",
    "        train[variable + '_' + label] = np.where(\n",
    "            train[variable] == label, 1, 0)\n",
    "        \n",
    "        test[variable + '_' + label] = np.where(\n",
    "            test[variable] == label,1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Yêu cầu 5\n",
    "# thiết lập StandardScaler để nó loại bỏ mean\n",
    "# nhưng không chia cho độ lêch chuẩn\n",
    "\n",
    "## VIẾT CODE Ở ĐÂY:\n",
    "scaler_mean = StandardScaler(with_mean=True, with_std=False)\n",
    "\n",
    "# thiết lập robustscaler để nó KHÔNG loại median\n",
    "# nhưng chuẩn hóa bằng max()-min(), quan trọng với\n",
    "# phạm vi quantile từ 0-100, thể hiện min và max\n",
    "\n",
    "## VIẾT CODE Ở ĐÂY:\n",
    "scaler_minmax = RobustScaler(with_centering=False,\n",
    "                             with_scaling=True,\n",
    "                             quantile_range=(0, 100))\n",
    "\n",
    "# khớp scaler với tập huấn luyện để nó học các tham số\n",
    "## VIẾT CODE Ở ĐÂY:\n",
    "scaler_mean.fit(X_train)\n",
    "scaler_minmax.fit(X_train)\n",
    "\n",
    "# biến đổi tập huấn luyện và tập kiểm tra\n",
    "## VIẾT CODE Ở ĐÂY:\n",
    "X_train_scaled = scaler_minmax.transform(scaler_mean.transform(X_train))\n",
    "X_test_scaled = scaler_minmax.transform(scaler_mean.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # tập hợp tất cả tên của các cột tương quan tập hợp tất cả tên của các cột tương quan\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # cần quan tâm tới các giá trị hệ số tuyệt đối \n",
    "                colname = corr_matrix.columns[i]  # lấy tên cột \n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    "\n",
    "corr_features = correlation(X_train, 0.8)\n",
    "print('correlated features: ', len(set(corr_features)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, r2_score\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs = SFS(RandomForestRegressor(n_estimators=10, n_jobs=4, random_state=10), \n",
    "           k_features=20, \n",
    "           forward=False, \n",
    "           floating=False, \n",
    "           verbose=2,\n",
    "           scoring='r2',\n",
    "           cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, r2_score\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold, f_classif, SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92, 90)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "a = np.random.randn(90,92)\n",
    "b = np.random.randn(92,1)\n",
    "c = np.dot(a,b)\n",
    "print(a.shape[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-a667d480eddd>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-a667d480eddd>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    a x b\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "a x b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 1)\n"
     ]
    }
   ],
   "source": [
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n",
    "                          'foo', 'bar'],\n",
    "                   'B' : ['one', 'one', 'two', 'three',\n",
    "                          'two', 'two'],\n",
    "                   'C' : [1, 5, 5, 2, 5, 5],\n",
    "                   'D' : [4, 5., 8., 1., 2., 9.]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2.000000\n",
       "1    2.236068\n",
       "2    2.828427\n",
       "3    1.000000\n",
       "4    1.414214\n",
       "5    3.000000\n",
       "Name: D, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"D\"].map(np.sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-8e2f524d6674>:1: FutureWarning: Dropping invalid columns in DataFrameGroupBy.transform is deprecated. In a future version, a TypeError will be raised. Before calling .transform, select only columns which should be valid for the transforming function.\n",
      "  df.groupby([\"A\"]).transform(lambda x: (x - x.mean()))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.666667</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          C    D\n",
       "0 -2.666667  NaN\n",
       "1  1.000000  0.0\n",
       "2  1.333333  3.0\n",
       "3 -2.000000 -4.0\n",
       "4  1.333333 -3.0\n",
       "5  1.000000  4.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"A\"]).transform(lambda x: (x - x.mean()))\n",
    "df[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.selection import DropDuplicateFeatures\n",
    "sel = DropDuplicateFeatures()\n",
    "sel.fit(X_4)\n",
    "X_5 = sel.transform(X_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.selection import DropCorrelatedFeatures, SmartCorrelatedSelection\n",
    "sel = DropCorrelatedFeatures(\n",
    "    threshold=0.8,\n",
    "    method='pearson',\n",
    "    missing_values='ignore'\n",
    ")\n",
    "\n",
    "\n",
    "# find correlated features\n",
    "\n",
    "sel.fit(X_train)\n",
    "X_train = sel.transform(X_train)\n",
    "X_test = sel.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy.stats as stats\n",
    "from feature_engine.selection import DropCorrelatedFeatures\n",
    "from feature_engine.selection import DropDuplicateFeatures\n",
    "from feature_engine.selection import RecursiveFeatureAddition\n",
    "from feature_engine.selection import RecursiveFeatureElimination\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, SGDRegressor, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, AdaBoostClassifier, AdaBoostRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler,  OneHotEncoder, OrdinalEncoder,PolynomialFeatures,PowerTransformer, FunctionTransformer\n",
    "from sklearn.svm import SVC, SVR \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, roc_auc_score, mean_squared_error, accuracy_score,confusion_matrix, f1_score\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76f5fe39443e87364fb9f87df6c450c683a0d5c6e22d011811809953e89ae77d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('myproject': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
